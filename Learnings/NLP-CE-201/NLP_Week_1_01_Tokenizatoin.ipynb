{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Week-1-01-Tokenizatoin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loganathan001/AI/blob/master/Learnings/NLP-CE-201/NLP_Week_1_01_Tokenizatoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Q0EMUl-1t4",
        "colab_type": "code",
        "outputId": "5a86f3a5-b066-481e-d9c8-93410aa3df9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "setup_google_colab.setup_week1()  \n",
        "# setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_project()\n",
        "# setup_google_colab.setup_honor()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-09 17:15:47--  https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2330 (2.3K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "\rsetup_google_colab.   0%[                    ]       0  --.-KB/s               \rsetup_google_colab. 100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-09 17:15:48 (32.8 MB/s) - ‘setup_google_colab.py’ saved [2330/2330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mds3MdzkYbUK",
        "colab_type": "text"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfSWgTjM_TDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCeGUtm_Y6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = \"This is Andrew's text, isn't it?\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcGgHLXdAFFR",
        "colab_type": "code",
        "outputId": "6dc3e62c-b7f5-4290-f330-148226bf21fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "tokenizer.tokenize(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', \"Andrew's\", 'text,', \"isn't\", 'it?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h_aJ7SLAlCT",
        "colab_type": "code",
        "outputId": "70a9a176-68fc-4bfc-8829-7e6681a668ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "tokenizer.tokenize(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'Andrew', \"'\", 's', 'text', ',', 'isn', \"'\", 't', 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t854YQYfA8MT",
        "colab_type": "code",
        "outputId": "45d4de34-d3b8-4c7a-cf09-b81912146d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokenizer.tokenize(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'Andrew', \"'s\", 'text', ',', 'is', \"n't\", 'it', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqZscKTXYS6i",
        "colab_type": "text"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v87uYv4WYeqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7817ad02-0f3a-43b8-cfd2-94316097c768"
      },
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "print(stemmer.stem(\"dresses\"))\n",
        "print(stemmer.stem(\"ponies\"))\n",
        "print(stemmer.stem(\"cats\"))\n",
        "print(stemmer.stem(\"feet\"))\n",
        "print(stemmer.stem(\"wolves\"))\n",
        "print(stemmer.stem(\"talked\"))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dress\n",
            "poni\n",
            "cat\n",
            "feet\n",
            "wolv\n",
            "talk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_LnGNmJaAvX",
        "colab_type": "text"
      },
      "source": [
        "Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOfgheVyZviv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "551bdc67-fb2f-4589-957d-9a4c1fd49efd"
      },
      "source": [
        "#nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"dresses\"))\n",
        "print(lemmatizer.lemmatize(\"ponies\"))\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"feet\"))\n",
        "print(lemmatizer.lemmatize(\"wolves\"))\n",
        "print(lemmatizer.lemmatize(\"talked\"))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "dress\n",
            "pony\n",
            "cat\n",
            "foot\n",
            "wolf\n",
            "talked\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}